<!DOCTYPE html>
<!--
    Plain-Academic by Vasilios Mavroudis
    Released under the Simplified BSD License/FreeBSD (2-clause) License.
    https://github.com/mavroudisv/plain-academic
-->
<style type="text/css">
.abstract-content {
    display: none;
    padding: 5px;
}
</style>

<html lang="en">
<head>
  <title>ICDM 2022 Workshop on Foundation Models in Vision and Language</title>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.6/css/bootstrap.min.css" />
  <link rel="stylesheet" href="css/simple.css" />
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.12.0/jquery.min.js"></script>
  <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.6/js/bootstrap.min.js"></script>
  <link href='https://fonts.googleapis.com/css?family=Oswald:700' rel='stylesheet' type='text/css' />
  <style>
      .center-cropped {
        object-fit: cover;
        object-position: center;
        width: 100%;
        height: 50%;
      }
      .fig {
        text-align: center;
        display: inline-block;
        width: 25%;
        vertical-align: top;
      }
      .navbar {
        max-width: 1370px;
        width: 100%;
        display: inline-block;
        left: 50%; 
        transform: translateX(-50%);
      }
  </style>
</head>
<body>

    <div class="header">
        <div class="headertext">
            <div class="title">ICDM 2022 Workshop</div>
            <div class="title title-bold">Foundation Models in Vision and Language</div>
        </div>
    </div>
<!-- Navigation -->
	<nav class="navbar navbar-inverse navbar-static-top" role="navigation">
	  <div class="container">
		<div class="navbar-header">
		  <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1">
						<span class="sr-only">Toggle navigation</span>
						<span class="icon-bar"></span>
						<span class="icon-bar"></span>
						<span class="icon-bar"></span>
					</button>
		</div>

		<!-- Collect the nav links, forms, and other content for toggling -->
		<div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
		  <ul class="nav navbar-nav">
				  <li><a href="#overview">Workshop Overview</a></li>
				  <!-- <li><a href="#schedule">Schedule</a></li> -->
			  	<li><a href="subpage/schedule.html">Schedule</a></li>
				  <li><a href="#speakers">Invited Speakers</a></li> 
				  <li><a href="#organizers">Organizers</a></li> 
          <li><a href="subpage/callforpaper.html">Call for Paper</a></li>
		  </ul>
		</div>
	  </div>
	</nav>
  
  <!-- Page Content -->
    <div class="container">

        <div class="row">
            <!-- Entries Column -->
            <div class="col-md-12">
                <!-- <h2 id="overview">Workshop Overview</h2> -->
                <div style="font-family: sans-serif; font-size: 20px;">
                <div style="margin-top:1%; text-align:justify;">
                <h2 id="overview">Overview</h2>

		<p>State-of-the-art AI systems can learn directly from whatever information they perceive, without relying on heavily labeled data sets for guidance. Such easy-to-collect data provide a more flexible form of supervision and a more affordable solution to data scalabilility. By training big deep neural networks with a large number of parameters on such heterogeneous data, recent foundation models have shown great promises in generality and usability. For example:
      <ul style="font-family: sans-serif; font-size: 20px; margin-top:1%; text-align:justify; line-height: 30px;">
        <li>Language: <a href="https://arxiv.org/abs/1810.04805">BERT</a>, <a href="https://en.wikipedia.org/wiki/GPT-3">GPT family</a></li> 
        <li>Vision: <a href="https://arxiv.org/abs/2002.05709">SimCLR</a>, <a href="https://arxiv.org/abs/2111.06377">MAE</a></li> 
        <li>Vision-and-Language: <a href="https://openai.com/blog/clip/">CLIP</a>, <a href="https://ai.googleblog.com/2021/05/align-scaling-up-visual-and-vision.html">ALIGN</a> and <a href="https://openai.com/dall-e-2/">DALLE</a></li>
      </ul>
    </p>
        <p>One appealing property of these foundation models is their ashtonishing performance on zero-shot and few-shot adaptation to various new real-world tasks. We organize this "Foundation Models in Vision and Language (FOMO-VL)" workshop, aiming to gather academic and industry communities to work on foundation models to solve real-world problems, focusing on the challenge of building scalable AI models can learn from heterogeneous data to gain generalized task-level transfer ability. This year, our FOMO-VL workshop will be held (tentatively in a hybrid mode) in conjunction with <a href="https://icdm22.cse.usf.edu/">ICDM 2022</a>, Orlando, FL, USA.</p>
                </div>
                </div>
           </div>
           

           <div class="col-md-12">
                <h2 id="schedule">Important Dates</h2>
                <h3>
                <ul style="font-family: sans-serif; font-size: 20px; margin-top:3%; text-align:justify; line-height: 30px;">
                    <li>Workshop paper submission deadline: October 10, 2022</li>
                    <li>Workshop paper acceptance decision to authors: October 13, 2022</li>
                    <li>Workshop dates: November 28, 2022</li>
                </ul>
                </h3>
           </div>

	   <div class="col-md-12">
                <h2 id="submission">How to Submit</h2>
                  <p style="font-family: sans-serif; font-size: 20px; margin-top:3%; text-align:justify; line-height: 30px;">
                    Please submit your papers to the <a href="https://wi-lab.com/cyberchair/2022/icdm22/scripts/submit.php?subarea=S18&%20undisplay_detail=1&wh=/cyberchair/2022/icdm22/scripts/ws_submit.php"> Online Submision System </a>. Please refer to <a href="subpage/callforpaper.html"> Call for Papers </a> for details on the topics and other related information. Thanks for the support of Amazon, some cash awards will be made to best papers. We look forward to your excellent work!
                  </p>
           </div>

           <div class="col-md-12">
                <h2 id="speakers">Invited Speakers</h2>
                  <div class="row">

                  <div class="col-lg-3 fig">
                    <img src="img/danqi.jpeg" width="150px" height="auto" style="border-radius: 50%">
                    <figcaption><h4><a href="https://www.cs.princeton.edu/~danqic/">Danqi Chen</a><br>Princeton University</h4></figcaption>
		    <a class="abstract-toggle">[Bio & Talk Info]</a>
			<div class="abstract-content">
			<p><b>Danqi Chen</b> is an Assistant Professor of Computer Science at Princeton University and co-leads the Princeton NLP Group. Her recent research focuses on training, adapting, and understanding large language models, and developing scalable and generalizable NLP systems for question answering, information extraction, and conversational agents. Before joining Princeton, Danqi worked as a visiting scientist at Facebook AI Research. She received her Ph.D. from Stanford University (2018) and B.E. from Tsinghua University (2012), both in Computer Science. Danqi is a recipient of a Sloan Fellowship, a Samsung AI Researcher of the Year award, outstanding paper awards from ACL 2016, EMNLP 2017, and ACL 2022, and multiple research awards from industry.</p> 
				<p><b>Talk Title</b>: Building Language Models Based on Retrieval </p>
				<p><b>Abstract</b>: Large language models (LLMs) have utterly transformed the field of natural language processing. However, training LLMs comes at a massive financial and environmental cost, making them out of reach of academic research labs. Meanwhile, these models are costly to update and brittle in leaking private text data. In this talk, I will argue that retrieval-based language models are a promising way of scaling LMs and overcoming the above limitations. I will discuss recent developments of retrieval-based language models, compare their pros and cons, and show their benefits in interpretability, adaptability, and privacy. In particular, I will introduce a new training approach for retrieval-based language models called TRIME (TRaining with In-batch MEmories), which can train LMs to retrieve better from the text during inference.</p>
			</div>
			  <br>
                  </div><!-- /.col-lg-4 -->      

                  <div class="col-lg-3 fig">
                    <img src="img/xifeng.jpeg" width="150px" height="auto" style="border-radius: 50%">
                    <figcaption><h4><a href="https://sites.cs.ucsb.edu/~xyan/">Xifeng Yan</a><br>UC at Santa Barbara</h4></figcaption>
			<a class="abstract-toggle">[Bio & Talk Info]</a>
			<div class="abstract-content">
			<p><b>Xifeng Yan</b> is a professor at the University of California at Santa Barbara. He holds the Venkatesh Narayanamurti Chair of Computer Science. He received his Ph.D. degree in Computer Science from the University of Illinois at Urbana-Champaign in 2006. He was a research staff member at the IBM T. J. Watson Research Center between 2006 and 2008. His work is centered on knowledge discovery, knowledge bases, and conversational AI. His contribution can be found in data mining, database systems, natural language processing, and their applications in interdisciplinary areas. His works were extensively referenced, with over 23,000 citations per Google Scholar and thousands of software downloads. He received NSF CAREER Award, IBM Invention Achievement Award, ACM-SIGMOD Dissertation Runner-Up Award, IEEE ICDM 10-year Highest Impact Paper Award, 2022 PLDI Distinguished Paper Award, and 2022 VLDB Test of Time Award.</p> 
				<p><b>Talk Title</b>: Limitations of Language Models in Arithmetic Induction </p>
				<p><b>Abstract</b>: Recent work has shown that large pretrained Language Models (LMs) can not only perform remarkably well on a range of Natural Language Processing (NLP) tasks but also start improving on reasoning tasks. Surprisingly, we find that these models have limitations on certain basic symbolic manipulation tasks such as copy, reverse, and addition. When the total number of symbols or repeating symbols increases, the model performance drops quickly. We investigate the potential causes behind this phenomenon and examine a set of possible methods, including explicit positional markers, fine-grained computation steps, and LMs with callable programs. Experimental results show that none of these techniques can solve the simplest addition induction problem completely. In the end, we introduce LMs with tutor, which is demonstrated with every single step of teaching. By limiting the type of operations it can conduct, LMs with tutor is able to deliver 100% accuracy in situations of OOD for simple tasks, shedding new insights on the boundary of large LMs in induction.</p>
			</div>
			  <br>
                  </div><!-- /.col-lg-4 -->      
                        

                  <div class="col-lg-3 fig">
                    <img src="img/tengyu.jpeg" width="150px" height="auto" style="border-radius: 50%">
                    <figcaption><h4><a href="https://ai.stanford.edu/~tengyuma/">Tengyu Ma</a><br>Standford University</h4></figcaption>
			<a class="abstract-toggle">[Bio & Talk Info]</a>
			<div class="abstract-content">
			<p><b>Tengyu Ma</b> is an assistant professor of computer science and statistics at Stanford. His research interests broadly include topics in machine learning and algorithms, such as deep learning and its theory, (deep) reinforcement learning and its theory, representation learning, robustness, non-convex optimization, distributed optimization, and high-dimensional statistics.</p> 
				<p><b>Talk Title</b>: Toward understanding foundation models </p>
				<p><b>Abstract</b>: TBD</p>
			</div>
			  <br>
                  </div><!-- /.col-lg-4 -->      

                  <div class="col-lg-3 fig">
                    <img src="img/parcalabescu.jpeg" width="150px" height="auto" style="border-radius: 50%">
                    <figcaption><h4><a href="https://leti.blebli.de/index.html">Letitia Parcalabescu</a><br>University of Heidelberg</h4></figcaption>
			<a class="abstract-toggle">[Bio & Talk Info]</a>
			<div class="abstract-content">
			<p><b>Letitia Parcalabescu</b> After having studied Physics and Computer Science, Letitia is a PhD candidate at Heidelberg University in the Heidelberg Natural Language Processing Group. Her research focuses on vision and language integration in multimodal machine learning. Her side-project revolves around the "AI Coffee Break with Letitia" YouTube channel, where the animated Ms. Coffee Bean explains and visualizes concepts from the latest natural language processing, computer vision and multimodal research.</p> 
				<p><b>Talk Title</b>: VALSE: Phenomenon-centered testing of Vision and Language models </p>
				<p><b>Abstract</b>: In this talk, I will introduce Vision and Language (VL) models by (a) explaining what pretrained VL models are, (b) giving a short overview over the architecture of VL models, and (c) stating their contributions towards vision and language integration with neural networks. I will present our recent work on the new VALSE benchmark to assess the degree of success of VL integration. VL models are usually evaluated on tasks such as image-sentence alignment or visual question answering. While performance on these tasks is important, task-centered evaluation does not disentangle the fine-grained linguistic capabilities of VL models. By contrast, VALSE comprises a suite of six specific linguistic phenomena grounded in the visual modality. Our zero-shot experiments for five widely-used pretrained V&L models on VALSE---CLIP, LXMERT, VisualBERT, ViLBERT and ViLBERT 12-in-1--- suggest that current VL models have considerable difficulty addressing most phenomena.</p>
			</div>
			  <br>
        <br>
                  </div><!-- /.col-lg-4 -->
		   </div>
		   
	          <div class="row">
		  <div class="col-lg-3 fig">
                    <img src="img/jason.jpeg" width="150px" height="auto" style="border-radius: 50%">
                    <figcaption><h4><a href="https://research.google/people/JasonBaldridge/">Jason Baldridge</a><br>Google Brain</h4></figcaption>
			<a class="abstract-toggle">[Bio & Talk Info]</a>
			<div class="abstract-content">
			<p><b>Jason Baldridge</b> is a research scientist at Google, where he works on grounded language understanding. He was previously an Associate Professor of Computational Linguistics at the University of Texas at Austin. His most recent work has focused on vision-and-language navigation, text-and-image representation learning and generating images from language descriptions. Jason received his Ph.D. from the University of Edinburgh in 2002, where his doctoral dissertation on Multimodal Combinatory Categorial Grammar was awarded the 2003 Beth Dissertation Prize from the European Association for Logic, Language and Information.</p> 
				<p><b>Talk Title</b>: What's Missing in Text-to-Image Generation? Current Models and Paths Forward </p>
				<p><b>Abstract</b>: Text-to-image models have made quite remarkable and surprising progress in just the past year. I'll provide some broader context for research in this area, focusing especially on WordsEye (from 2000) and Google's recent Parti model. I'll highlight important failure modes of modern models in accurately depicting detailed textual descriptions and argue we need to be more precise in defining and developing description-to-depiction tasks as we consider data and modeling strategies that will allow the next generation of models to better represent descriptions and reflect them visually.</p>
			</div>
			  <br>
                  </div><!-- /.col-lg-4 -->
			  
		  <div class="col-lg-3 fig">
                    <img src="img/lu.jpeg" width="150px" height="auto" style="border-radius: 50%">
                    <figcaption><h4><a href="https://www.microsoft.com/en-us/research/people/luyuan/">Lu Yuan</a><br>Microsoft Cloud and AI</h4></figcaption>
			 <a class="abstract-toggle">[Bio & Talk Info]</a>
			<div class="abstract-content">
			<p><b>Lu Yuan</b> is a Principal Research Manager in the Cognitive Services Research at Microsoft Cloud and AI, Bellevue, WA. He got his MS degree from the Department of Electronic Engineering at Tsinghua University in 2005 and his Ph.D degree from Department of Computer Science & Engineering at the Hong Kong University of Science and Technology (HKUST) in 2009. Then, he joined Visual Computing Group at Microsoft Research Asia in August, 2009. His current research interests include: computer vision, large-scale pretraining, human understanding and computational photography.</p> 
				<p><b>Talk Title</b>: Florence: A New Computer Vision Foundation Model </p>
				<p><b>Abstract</b>: In this talk, I will introduce Florence, a new computer vision foundation model, which is trained on noisy web-scale dataset and demonstrates outstanding performance in a wide range of diverse downstream tasks and many types of transfer learning. It is critical for this mission to solve real-world computer vision applications. To develop the foundation model, we incorporate the latest hierarchical transformer architecture as encoders, and leverage unified contrastive learning to learn a good image-text embedding which empowers generic image understanding capability. The purpose of building foundation model is not solely improving the performance in close-world recognition tasks. The most important thing is that it is pushing the perception toward the open world and cognition. I will showcase how Florence enables open vocabulary recognition, self-evolving learning, and reasoning as humans do. Florence is designed to pave the way for building vision foundation models to power millions of real-world vision tasks and applications. In addition, the preliminary progress may motivate more research to build any breakthroughs for integrative AI. </p>
			</div>
			  <br>
                  </div><!-- /.col-lg-4 -->
			  
		  <div class="col-lg-3 fig">
                    <img src="img/jiasen.jpeg" width="150px" height="auto" style="border-radius: 50%">
                    <figcaption><h4><a href="https://scholar.google.com/citations?user=zP9K32EAAAAJ&hl=en">Jiasen Lu</a><br>Allen Institute of Artificial Intelligence</h4></figcaption>
			<a class="abstract-toggle">[Bio & Talk Info]</a>
			<div class="abstract-content">
			<p><b>Jiasen Lu</b>  is a research scientist in Prior at Allen Institute for AI, Seattle. His research interests span in computer vision, vision & language. Prior to joining AI2 at Feb, 2020, he earned his Ph.D. in Computer Science from School of Interactive Computing at Georgia Tech with thesis “Visually Grounded Language understanding and Generation”.</p> 
				<p><b>Talk Title</b>: Unified-IO: A Unified Model for Vision, Language and Multi-Modal Tasks </p>
				<p><b>Abstract</b>: In this talk, I will talk about Unified-IO, which is the first neural model to perform a large and diverse set of AI tasks spanning classical computer vision, image synthesis, vision-and-language, and natural language processing (NLP). Unified-IO achieves this broad unification by homogenizing every task's input and output into a sequence of tokens drawn from a discrete and finite vocabulary. </p>
			</div>
			  <br>
                  </div><!-- /.col-lg-4 -->
			  
		  <div class="col-lg-3 fig">
                    <img src="img/junyang.jpeg" width="150px" height="auto" style="border-radius: 50%">
                    <figcaption><h4><a href="https://justinlin610.github.io/">Justin Lin</a><br>DAMO Academy, Alibaba Group</h4></figcaption>
			<a class="abstract-toggle">[Bio & Talk Info]</a>
			<div class="abstract-content">
			<p><b>Junyang Lin</b> is a staff engineer at DAMO Academy, Alibaba Group. He received his master degree at Peking University in 2019. His research interests include multimodal representation learning, natural language processing, and computer vision, with a focus on large-scale pretraining. He has been in. charge of developing large-scale unified multimodal pretrained models, including M6, OFA, etc., and also the real-world applications of foundation models.</p> 
				<p><b>Talk Title</b>: Towards a One-For-All System for Multimodal Multitask Learning </p>
				<p><b>Abstract</b>: In recent years, mirroring large language models, multimodal pretraining that incorporates multimodal representation learning and large-scale pretraining has been developing rapidly and creating a series of new state-of-the-art performances in vision-language downstream tasks and even vision tasks. A trend this year is the unified models that enable multimodal multitask learning. They achieve both the unification of tasks as well as modalities and outstanding performance. This talk first reviews the developments of multimodal pretraining, and then provides an introduction to the One-For-All (OFA) model for vision-language pretraining and downstream transfer, and also a recently introduced framework OFASys that facilitates multimodal multitask learning with a simple interface. We hope this can promote the research in building generalist models. </p>
			</div>
			  <br>
                  </div><!-- /.col-lg-4 -->
              </div>
           </div>

	    <div class="col-md-12">
                <h2 id="advising">Advisory Committee -- Panelists</h2>
                  <div class="row">

                  <div class="col-lg-3 fig">
                    <img src="https://www.microsoft.com/en-us/research/uploads/prod/2020/09/20191101_170350330_iOS-4.jpg" width="140px" height="auto" style="border-radius: 50%">
                    <figcaption><h4><a href="https://www.microsoft.com/en-us/research/people/jfgao/">Jianfeng Gao</a><br>Microsoft Research, Redmond</h4></figcaption>
                </div><!-- /.col-lg-4 -->      
                        


                  <div class="col-lg-3 fig">
                    <img src="img/trishul.jpeg" width="150px" height="auto" style="border-radius: 50%">
                    <figcaption><h4><a href="https://scholar.google.com/citations?user=DrNeo_0AAAAJ&hl=en">Trishul Chilimbi</a><br>Amazon</h4></figcaption>
                </div><!-- /.col-lg-4 -->      
                        

                  <div class="col-lg-3 fig">
                    <img src="https://pbs.twimg.com/profile_images/550901065751592963/J3wFUStk_400x400.jpeg" width="150px" height="auto" style="border-radius: 50%">
                    <figcaption><h4><a href="https://www.cs.cmu.edu/~rsalakhu/">Ruslan Salakhutdinov</a><br>Carnegie Mellon University</h4></figcaption>
                </div><!-- /.col-lg-4 -->   

                  <div class="col-lg-3 fig">
                    <img src="img/Christoph.jpg" width="155px" height="auto" style="border-radius: 50%">
                    <figcaption><h4><a href="https://laion.ai/team/">Christoph Schuhmann</a><br>LAION</h4></figcaption>  
                  </div><!-- /.col-lg-4 -->
			  
		  <div class="col-lg-3 fig">
                    <img src="img/schmidt.jpeg" width="155px" height="auto" style="border-radius: 50%">
                    <figcaption><h4><a href="https://people.csail.mit.edu/ludwigs/">Ludwig Schmidt</a><br> University of Washington, OpenCLIP </h4></figcaption>  
                  </div><!-- /.col-lg-4 -->
                    
		  <div class="col-lg-3 fig">
                    <img src="https://norouzi.github.io/img/Mohammad_Norouzi.jpg" width="155px" height="auto" style="border-radius: 50%">
                    <figcaption><h4><a href="https://norouzi.github.io/">Mohammad Norouzi</a><br> Google Brain </h4></figcaption>  
                  </div><!-- /.col-lg-4 -->
                  </div>
           </div>
	    
	<div class="row">
	    <h2 id="accepted">Accepted Papers</h2>
            <div class="large-12 columns" style="text-align:left;">
                <p style="font-size:15px; font-weight: 400; text-align:left">
                    <ul style="font-size:15px;">
                        The following papers were accepted to the FOMO-VL Workshop:
                        <br>
                        <br>
                    
                        <li style="margin-left:30px">
                            <b> <a href=###>TBD  </a></b> TBD
                        </li>
                        <li style="margin-left:30px">
                            <b> <a href=###>TBD </a> </b> TBD
                        </li>
                    </ul>    
                </p>
            </div>
        </div>
	    
        <div class="col-md-12">
                <h2 id="organizers">Organizers</h2>
                <div class="row">
                  <div class="col-lg-3 fig">
                      <img src="https://cse.buffalo.edu/~changyou/images/ccy1.jpg" width="150px" height="auto" style="border-radius: 50%">
                      <figcaption><h4><a href="https://cse.buffalo.edu/~changyou/index.html">Changyou Chen</a><br>University at Buffalo, Amazon</h4></figcaption>
                  </div><!-- /.col-lg-4 -->
                  <div class="col-lg-3 fig">
                      <img src="https://pbs.twimg.com/profile_images/1512443346077188098/9Jg9eWHn_400x400.jpg" width="150px" height="auto" style="border-radius: 50%">
                      <figcaption><h4><a href="https://chunyuan.li/">Chunyuan Li</a><br>Microsoft Research, Redmond</h4></figcaption>
                  </div><!-- /.col-lg-4 -->
                  <div class="col-lg-3 fig">
                    <img src="https://jiahuiyu.com/resources/files/Jiahui_Yu.jpeg" width="150px" height="auto" style="border-radius: 50%">
                    <figcaption><h4><a href="https://jiahuiyu.com/">Jiahui Yu</a><br>Google Brain</h4></figcaption>
                </div><!-- /.col-lg-4 -->                  
                  <div class="col-lg-3 fig">
                      <img src="img/hongxia_yang.jpg" width="150px" height="auto" style="border-radius: 50%">
                      <figcaption><h4><a href="https://sites.google.com/site/hystatistics/">Hongxia Yang</a><br>Alibaba Group</h4></figcaption>
                  </div><!-- /.col-lg-4 -->
                  <div class="col-lg-3 fig">
                      <img src="https://www.cs.cmu.edu/~pliang/images/photo.jpeg" width="150px" height="auto" style="border-radius: 50%">
                      <figcaption><h4><a href="https://www.cs.cmu.edu/~pliang/">Paul Pu Liang</a><br>Carnegie Mellon University</h4></figcaption>
                  </div><!-- /.col-lg-4 -->
                  <div class="col-lg-3 fig">
                      <img src="img/yi_xu.jpeg" width="150px" height="auto" style="border-radius: 50%">
                      <figcaption><h4><a href="https://www.linkedin.com/in/yeahgoyixu">Yi Xu</a><br>Amazon</h4></figcaption>
                  </div><!-- /.col-lg-4 -->
                  <div class="col-lg-3 fig">
                      <img src="img/sontran.jpeg" width="150px" height="auto" style="border-radius: 50%">
                      <figcaption><h4><a href="https://www.linkedin.com/in/sontran">Son Tran</a><br>Amazon</h4></figcaption>
                  </div><!-- /.col-lg-4 -->
                  <div class="col-lg-3 fig">
                      <img src="img/belinda_zeng.jpeg" width="150px" height="auto" style="border-radius: 50%">
                      <figcaption><h4><a href="https://www.linkedin.com/in/belindazeng">Belinda Zeng</a><br>Amazon</h4></figcaption>
                  </div><!-- /.col-lg-4 -->
                </div>

            </div>
        </div>

    </div>
    <!-- /.container -->
    
    <!-- Other people may like it too! -->
    <a style="color:#b5bec9;font-size:0.8em; float:right;" href="https://github.com/mavroudisv/plain-academic">Plain Academic</a> 

    <footer style="background-color:#444; color:#fdfdfd; padding:40px 0;">
      <p style="text-align:center; font-family: sans-serif; font-size: 14px;">
      The website design is helped by <a href="https://www.linkedin.com/in/sifan-wang-99535012b/">Sifan Wang @Microsoft</a>  <br/>
      Have any questions or suggestions on the workshop?
      Feel free to reach us at: <br/>
      <a href="https://github.com/fomo-vl">https://github.com/fomo-vl</a><br/>
      </p>
  </footer>
    
 <script>
    $(document).foundation();
 </script>
<script>
    (function(i, s, o, g, r, a, m) {
        i['GoogleAnalyticsObject'] = r;
        i[r] = i[r] || function() {
            (i[r].q = i[r].q || []).push(arguments)
        }, i[r].l = 1 * new Date();
        a = s.createElement(o),
            m = s.getElementsByTagName(o)[0];
        a.async = 1;
        a.src = g;
        m.parentNode.insertBefore(a, m)
    })(window, document, 'script', '//www.google-analytics.com/analytics.js', 'ga');

    ga('create', 'UA-63638588-1', 'auto');
    ga('send', 'pageview');
    </script>
    <!-- jquery smooth scroll to id's -->
    <script>
    $(function() {
        $('a[href*=#]:not([href=#])').click(function() {
            if (location.pathname.replace(/^\//, '') == this.pathname.replace(/^\//, '') && location.hostname == this.hostname) {
                var target = $(this.hash);
                target = target.length ? target : $('[name=' + this.hash.slice(1) + ']');
                if (target.length) {
                    $('html,body').animate({
                        scrollTop: target.offset().top
                    }, 1000);
                    return false;
                }
            }
        });
    });
    </script>
    <script>
        $(".abstract-toggle").click(function () {

        $header = $(this);
        //getting the next element
        $content = $header.next();
        //open up the content needed - toggle the slide- if visible, slide up, if not slidedown.
        $content.slideToggle(500, function () {
            //execute this after slideToggle is done
            //change text of header based on visibility of content div
            $header.text(function () {
                //change text based on condition
                // return $content.is(":visible") ? "[Collapse]" : "[Expand]";
                return $content.is(":visible") ? "Collapse" : "[Bio & Talk Info]";
            });
        });

        });
    </script>
	
</body>

</html>
